import torch

from torch import nn, tensor
from torch.nn import functional as F
from torchvision import models
from typing import Union, Tuple

from utils.data import getGPT2Tokenizer


class ImageEncoder(nn.Module):
    """
    
    Wrapper around pretrained ResNet50 model to extract features from images

    Attributes:
        extractor_spatial_dim (int):            Output spacial dimension of ResNet50 model
        extractor_pixel_dim (int):              Output pixel dimensions of ResNet50 model
        feature_extractor (models.resnet50):    Pretrained ResNet50 model
        feature_reduction (nn.Sequential):      Linear layer with ReLU activation
    
    """

    def __init__(self, attention_dim: int) -> None:
        """
        
        Constructor for ImageEncoder

        
        Parameters:
            attention_dim (int):    The dimension of the attention layer of the decoder
        
        """

        super(ImageEncoder, self).__init__()

        # Output dimension of the ResNet50 model without the final
        # pooling or fully connected layer
        self.extractor_spatial_dim = 2048
        self.extractor_pixel_dim = 7 * 7

        # Initialize ResNet50 model with default weights
        resnet_model = models.resnet50(weights='DEFAULT')
        
        # Remove fully connected layer and pooling layer
        modules = list(resnet_model.children())[:-2]

        # Create feature extraction model using remaining modules or ResNet50 model
        self.feature_extractor = nn.Sequential(*modules)

        # Create linear layer to reduce spacial dimension of image to `attention_dim`
        self.feature_reduction = nn.Sequential(
            nn.Linear(self.extractor_spatial_dim, attention_dim),
            nn.ReLU(inplace=True)
        )


    def forward(self, images: tensor) -> tensor:
        """
        
        Handles the forward pass for ImageEncoder

        
        Parameters:
            images (tensor):    Batch of tensors containing image representations

        
        Returns:
            tensor:     A tensor representing the scores generated by the ResNet50 model

        """

        # Extract features from images using pretrained model
        with torch.no_grad():
            features = self.feature_extractor(images)

        # Reshape features to shape required be reduction layer
        reshaped_features = features.view(features.size(0), self.extractor_pixel_dim, self.extractor_spatial_dim)

        return self.feature_reduction(reshaped_features)
    


class LuongAttention(nn.Module):
    """
    
    Implementation of Luong Attention layer based on description in TensorFlow docs
    
    """
    def __init__(self) -> None:
        """
        
        Constructor for LuongAttention
        
        """

        super(LuongAttention, self).__init__()


    def forward(self, query: tensor, value: tensor, key:Union[tensor, None] = None) -> tensor:
        """
        
        Handles the forward pass for LuongAttention layer

        
        Parameters:
            query (tensor):             Batch of tensors representing current decoder hidden state
            value (tensor):             Batch of tensors representing encoder output
            key Union[tensor, None]:    Batch of tensors representing encoder output

        
        Returns:
            tensor:     A tensor representing the context vector from the attention layer

        """
        # Set key tensor to be value tensor if no key is passed in
        if key is None:
            key = value

        # Compute the softmax distribution of Dot-Product between queries and keys
        softmax_distribution = F.softmax(torch.bmm(query, key.transpose(1, 2)), dim=-1)

        # Compute linear combination between softmax distribution and value
        return torch.bmm(softmax_distribution, value)

        

class CaptionDecoder(nn.Module):
    """
    
    Decoder model to generate tokens from image features and captions

    Attributes:
        embedding (nn.Module):          Embedding layer to convert inputs into embeddings
        LSTM (nn.Module):               LSTM layer to process embeddings into hidden state
        attention (nn.Module):          Attention layer to compute context vector
        layer_norm (nn.Module):         Layer norm to normalize the inputs across features
        decoder_linear (nn.Module):     Linear layer to transform features into scores
    
    """
    def __init__(self, attention_dim: int) -> None:
        """
        
        Constructor for CaptionDecoder

        
        Parameters:
            attention_dim (int):    Integer representing attention dimension
        
        """

        super(CaptionDecoder, self).__init__()

        # Initialize tokenizer used for tokenizing inputs
        tokenizer = getGPT2Tokenizer()

        # Initialize an embedding layer
        self.embedding = nn.Embedding(len(tokenizer), attention_dim, padding_idx=tokenizer.pad_token_id)

        # Initialize the LSTM layer for sequences
        self.lstm = nn.LSTM(attention_dim, attention_dim, batch_first=True)

        # Initialize custom LuongAttention layer and layer normalization
        self.attention = LuongAttention()
        self.layer_norm = nn.LayerNorm(attention_dim)

        # Initialize linear layer to decode previous steps into vocabulary shape
        self.decoder_linear = nn.Linear(attention_dim, len(tokenizer))


    def forward(self, captions: tensor, encoder_output: tensor, prev_lstm_states: Tuple[Union[tensor, None], Union[tensor, None]] = (None, None)) -> tuple[tensor, tensor]:
        """
        
        Handles the forward pass for CaptionDecoder

        
        Parameters:
            captions (tensor):                                                  Batch of tensors representing the input captions
            encoder_output (tensor):                                            Batch of tensors representing the output of the encoder model
            prev_lstm_state (Tuple[Union[tensor, None], Union[tensor, None]):   Tensor representing LSTM state of previous time step

        
        Returns:
            tensor:                     A tensor representing the scores for each token
            Tuple[tensor, tensor]:      A tuple of tensors representing the hidden state and cell state of the LSTM layer

        """

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # Set hidden state and cell state for LSTM to zero if (None, None) is given
        if prev_lstm_states == (None, None):
            prev_lstm_states = (torch.zeros(1, encoder_output.size(0), encoder_output.size(-1)).to(device),
                               torch.zeros(1, encoder_output.size(0), encoder_output.size(-1)).to(device))

        # Generate embeddings from input captions
        embeddings = self.embedding(captions)

        # Generate features and hidden state for captions
        lstm_output, lstm_states = self.lstm(embeddings, prev_lstm_states)

        # Derive context vector from attention layer
        context_vector = self.attention(lstm_output, encoder_output)

        # Add output and context vector
        addition = lstm_output + context_vector

        # Normalize features
        layer_norm = self.layer_norm(addition)

        # Generate output scores over size of vocabulary
        decoder_output = self.decoder_linear(layer_norm)

        return decoder_output, lstm_states


class ImageCaptioningNetwork(nn.Module):
    """
    
    Full network architecture used to extract features from images and
    generate captions using extracted features

    Attributes:
        encoder (ImageEncoder):     ResNet50 model to extract features from images
        decoder (CaptionDecoder):   LSTM network used to generate captions using image features
    
    """
    def __init__(self, attention_dim: int) -> None:
        """
        
        Constructor for ImageCaptioningNetwork

        
        Parameters:
            attention_dim (int):    Integer representing attention dimension

        """

        super(ImageCaptioningNetwork, self).__init__()

        # Initialize encoder and decoder for caption generation pipeline
        self.encoder = ImageEncoder(attention_dim)
        self.decoder = CaptionDecoder(attention_dim)


    def forward(self, images: tensor, captions: tensor, prev_lstm_state: Tuple[Union[tensor, None], Union[tensor, None]] = (None, None)) -> Tuple[tensor, Tuple[tensor, tensor]]:
        """
        
        Handles the forward pass for ImageCaptioningNetwork

        
        Parameters:
            images (tensor):                            Batch of tensors containing image representations
            captions (tensor):                          Batch of tensors containing captions
            prev_lstm_state (Union[tensor, None]):      Tensor representing LSTM state of previous time step

        
        Returns:
            tensor:                     A tensor representing the scores generated by the model
            Tuple[tensor, tensor]:      A tuple of tensors representing the hidden state and cell state of the LSTM layer

        """

        encoder_output = self.encoder(images)
        decoder_output, lstm_state = self.decoder(captions, encoder_output, prev_lstm_state)

        return decoder_output, lstm_state
    

def loadModel(trained_model_path: str = None) -> ImageCaptioningNetwork:
    """
    
    Load a saved model's state dict if one is passed, otherwise load base model


    Parameters:
        trained_model_path (str):   Path to pretrained model

    
    Returns:
        ImageCaptioningNetwork: The image captioning model
    
    """

    # Initialize the base image captioning model
    model = ImageCaptioningNetwork(attention_dim=512)
    
    # Load state dict of pretrained model if it is passed in
    if trained_model_path:
        model.load_state_dict(torch.load(trained_model_path, weights_only=True))

    return model
