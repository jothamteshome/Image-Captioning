import torch

from torch import nn, tensor
from torch.nn import functional as F
from torchvision import models
from typing import Union


class ImageEncoder(nn.Module):
    """
    
    Wrapper around pretrained ResNet50 model to extract features from images

    Attributes:
        extractor_spatial_dim (int):            Output spacial dimension of ResNet50 model
        extractor_pixel_dim (int):              Output pixel dimensions of ResNet50 model
        feature_extractor (models.resnet50):    Pretrained ResNet50 model
        feature_reduction (nn.Sequential):      Linear layer with ReLU activation
    
    """

    def __init__(self, attention_dim: int) -> None:
        """
        
        Constructor for ImageEncoder

        
        Parameters:
            attention_dim (int):    The dimension of the attention layer of the decoder
        
        """

        super(ImageEncoder, self).__init__()

        # Output dimension of the ResNet50 model without the final
        # pooling or fully connected layer
        self.extractor_spatial_dim = 2048
        self.extractor_pixel_dim = 7 * 7

        # Initialize ResNet50 model with default weights
        resnet_model = models.resnet50(weights='DEFAULT')
        
        # Remove fully connected layer and pooling layer
        modules = list(resnet_model.children())[:-2]

        # Create feature extraction model using remaining modules or ResNet50 model
        self.feature_extractor = nn.Sequential(*modules)

        # Create linear layer to reduce spacial dimension of image to `attention_dim`
        self.feature_reduction = nn.Sequential(
            nn.Linear(self.extractor_spatial_dim, attention_dim),
            nn.ReLU(inplace=True)
        )


    def forward(self, images: tensor) -> tensor:
        """
        
        Handles the forward pass for ImageEncoder

        
        Parameters:
            images (tensor):    Batch of tensors containing image representations

        
        Returns:
            tensor:     A tensor representing the scores generated by the ResNet50 model

        """

        # Extract features from images using pretrained model
        with torch.no_grad():
            features = self.feature_extractor(images)

        # Reshape features to shape required be reduction layer
        reshaped_features = features.view(features.size(0), self.extractor_pixel_dim, self.extractor_spatial_dim)

        return self.feature_reduction(reshaped_features)
    


class LuongAttention(nn.Module):
    """
    
    Implementation of Luong Attention layer based on description in TensorFlow docs
    
    """
    def __init__(self) -> None:
        """
        
        Constructor for LuongAttention
        
        """

        super(LuongAttention, self).__init__()


    def forward(self, query: tensor, value: tensor, key:Union[tensor, None] = None) -> tensor:
        """
        
        Handles the forward pass for LuongAttention layer

        
        Parameters:
            query (tensor):             Batch of tensors representing current decoder hidden state
            value (tensor):             Batch of tensors representing encoder output
            key Union[tensor, None]:    Batch of tensors representing encoder output

        
        Returns:
            tensor:     A tensor representing the context vector from the attention layer

        """
        # Set key tensor to be value tensor if no key is passed in
        if key is None:
            key = value

        # Compute the softmax distribution of Dot-Product between queries and keys
        softmax_distribution = F.softmax(torch.bmm(query, key.transpose(1, 2)), dim=-1)

        # Compute linear combination between softmax distribution and value
        return torch.bmm(softmax_distribution, value)

        

class CaptionDecoder(nn.Module):
    """
    
    Decoder model to generate tokens from image features and captions

    Attributes:
        embedding (nn.Module):          Embedding layer to convert inputs into embeddings
        gru (nn.Module):                GRU layer to process embeddings into hidden state
        attention (nn.Module):          Attention layer to compute context vector
        layer_norm (nn.Module):         Layer norm to normalize the inputs across features
        decoder_linear (nn.Module):     Linear layer to transform features into scores
    
    """
    def __init__(self, vocab_size: int, attention_dim: int, padding_idx: int) -> None:
        """
        
        Constructor for CaptionDecoder

        
        Parameters:
            vocab_size (int):       Integer representing the vocabulary size
            attention_dim (int):    Integer representing attention dimension
            padding_idx (int):      Index for padding token
        
        """

        super(CaptionDecoder, self).__init__()

        # Initialize an embedding layer
        self.embedding = nn.Embedding(vocab_size, attention_dim, padding_idx=padding_idx)

        # Initialize the gru layer for sequences
        self.gru = nn.GRU(attention_dim, attention_dim, batch_first=True)

        # Initialize custom LuongAttention layer and layer normalization
        self.attention = LuongAttention()
        self.layer_norm = nn.LayerNorm(attention_dim)

        # Initialize linear layer to decode previous steps into vocabulary shape
        self.decoder_linear = nn.Linear(attention_dim, vocab_size)


    def forward(self, captions: tensor, encoder_output: tensor, prev_gru_state: Union[tensor, None]) -> tuple[tensor, tensor]:
        """
        
        Handles the forward pass for CaptionDecoder

        
        Parameters:
            captions (tensor):                      Batch of tensors representing the input captions
            encoder_output (tensor):                Batch of tensors representing the output of the encoder model
            prev_gru_state (Union[tensor, None]):   Tensor representing GRU state of previous time step

        
        Returns:
            tensor:     A tensor representing the scores for each token
            tensor:     A tensor representing the hidden state of the GRU layer

        """

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # Set hidden state for gru if none is given
        if prev_gru_state is None:
            prev_gru_state = torch.zeros(1, encoder_output.size(0), encoder_output.size(-1)).to(device)

        # Generate embeddings from input captions
        embeddings = self.embedding(captions)

        # Generate features and hidden state for captions
        gru_output, gru_state = self.gru(embeddings, prev_gru_state)

        # Derive context vector from attention layer
        context_vector = self.attention(gru_output, encoder_output)

        # Add output and context vector
        addition = gru_output + context_vector

        # Normalize features
        layer_norm = self.layer_norm(addition)

        # Generate output scores over size of vocabulary
        decoder_output = self.decoder_linear(layer_norm)

        return decoder_output, gru_state


class ImageCaptioningNetwork(nn.Module):
    """
    
    Full network architecture used to extract features from images and
    generate captions using extracted features

    Attributes:
        encoder (ImageEncoder):     ResNet50 model to extract features from images
        decoder (CaptionDecoder):   GRU network used to generate captions using image features
    
    """
    def __init__(self, vocab_size: int, attention_dim: int, padding_idx: int) -> None:
        """
        
        Constructor for ImageCaptioningNetwork

        
        Parameters:
            vocab_size (int):       Integer representing the vocabulary size
            attention_dim (int):    Integer representing attention dimension
            padding_idx (int):      Index for padding token

        """

        super(ImageCaptioningNetwork, self).__init__()

        # Initialize encoder and decoder for caption generation pipeline
        self.encoder = ImageEncoder(attention_dim)
        self.decoder = CaptionDecoder(vocab_size, attention_dim, padding_idx)


    def forward(self, images: tensor, captions: tensor, prev_gru_state: Union[tensor, None] = None) -> tensor:
        """
        
        Handles the forward pass for ImageCaptioningNetwork

        
        Parameters:
            images (tensor):    Batch of tensors containing image representations
            captions (tensor):  Batch of tensors containing captions
            prev_gru_state (Union[tensor, None]):   Tensor representing GRU state of previous time step

        
        Returns:
            tensor:     A tensor representing the scores generated by the model

        """

        encoder_output = self.encoder(images)
        decoder_output = self.decoder(captions, encoder_output, prev_gru_state)

        return decoder_output
    

def loadModel(vocab_size: int, padding_idx: int, trained_model_path: str = None) -> ImageCaptioningNetwork:
    """
    
    Load a saved model's state dict if one is passed, otherwise load base model


    Parameters:
        vocab_size (int):           Size of input vocabulary
        padding_idx (int):          Index for padding token
        trained_model_path (str):   Path to pretrained model

    
    Returns:
        ImageCaptioningNetwork: The image captioning model
    
    """

    # Initialize the base image captioning model
    model = ImageCaptioningNetwork(vocab_size=vocab_size,
                                   attention_dim=512,
                                   padding_idx=padding_idx)
    
    # Load state dict of pretrained model if it is passed in
    if trained_model_path:
        model.load_state_dict(torch.load(trained_model_path, weights_only=True))

    return model
